%% Machine Learning Online Class%  Exercise 1: Linear regression with multiple variables%%  Instructions%  ------------% %  This file contains code that helps you get started on the%  linear regression exercise. %%  You will need to complete the following functions in this %  exericse:%%     warmUpExercise.m%     plotData.m%     gradientDescent.m%     computeCost.m%     gradientDescentMulti.m%     computeCostMulti.m%     featureNormalize.m%     normalEqn.m%%  For this part of the exercise, you will need to change some%  parts of the code below for various experiments (e.g., changing%  learning rates).%%% Initialization%% ================ Part 1: Feature Normalization ================%% Clear and Close Figuresclear ; close all; clcfprintf('Loading data ...\n');%% Load Datadata = load('JestaTest2.txt');%data = load('TF8T_HPC8T.txt');data = [data data(:,2).^2 data(:,2).*data(:,3) data(:,2).*data(:,4) data(:,2).*data(:,5) data(:,2).*data(:,6) data(:,2).*data(:,7)];X = data(:, 2: length(data(1,:)));y = data(:, 1);m = length(y);%X = [X X(:,1).^2 X(:,1).*X(:,2) X(:,1).*X(:,3) X(:,1).*X(:,4) X(:,1).*X(:,5) X(:,1).*X(:,6)]FeatureSize = length(X(1,:));% Print out some data points%fprintf('First 10 examples from the dataset: \n');%fprintf(' x = [%.0f %.0f], y = %.0f \n', [X(1:10,:) y(1:10,:)]');%fprintf('Program paused. Press enter to continue.\n');%pause;% Scale features and set them to zero meanfprintf('Normalizing Features ...\n');[X mu sigma] = featureNormalize(X);% Add intercept term to XX = [ones(m, 1) X];%% ================ Part 2: Gradient Descent ================% ====================== YOUR CODE HERE ======================% Instructions: We have provided you with the following starter%               code that runs gradient descent with a particular%               learning rate (alpha). %%               Your task is to first make sure that your functions - %               computeCost and gradientDescent already work with %               this starter code and support multiple variables.%%               After that, try running gradient descent with %               different values of alpha and see which one gives%               you the best result.%%               Finally, you should complete the code at the end%               to predict the price of a 1650 sq-ft, 3 br house.%% Hint: By using the 'hold on' command, you can plot multiple%       graphs on the same figure.%% Hint: At prediction, make sure you do the same feature normalization.%fprintf('Running gradient descent ...\n');% Choose some alpha valuealpha = 0.01;num_iters = 500;% Init Theta and Run Gradient Descent theta = zeros(FeatureSize+1, 1);[theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);% Plot the convergence graph%figure;%plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);%xlabel('Number of iterations');%ylabel('Cost J');% Display gradient descent's resultfprintf('Theta computed from gradient descent: \n');fprintf(' %f \n', theta);fprintf('\n');fprintf('Program paused. Press enter to continue.\n');pause;%% ================ Part 3: Normal Equations ================fprintf('Solving with normal equations...\n');% ====================== YOUR CODE HERE ======================% Instructions: The following code computes the closed form %               solution for linear regression using the normal%               equations. You should complete the code in %               normalEqn.m%%               After doing so, you should complete this code %               to predict the price of a 1650 sq-ft, 3 br house.%%% Load Data% Calculate the parameters from the normal equationtheta = normalEqn(X, y);% Display normal equation's resultfprintf('Theta computed from the normal equations: \n');fprintf('Each iters cost: %f\n',((X*theta-y).^2)/2/m);fprintf('Total cost: %f.\n',computeCost(X, y, theta));fprintf(' %f \n', theta);fprintf('\n');fprintf('Begin PCA\n');pause;FeatureSize = length(data(1,:))-1;X = data(:, 2: length(data(1,:)));y = data(:, 1);m = length(y);[m, n] = size(X);[X, mu, sigmaAA] = featureNormalize(X);% You need to return the following variables correctly.U = zeros(n);S = zeros(n);V = zeros(n);K=2;sigma = X'*X/m;[U,S,V] = svd(sigma)Z = zeros(size(X, 1), K);Z = X*U(:,1:K);[Z, mu, sigmaAA] = featureNormalize(Z);Z_new = [ones(m,1) Z];theta = normalEqn(Z_new, y);fprintf('Total cost: %f.\n',computeCost(Z_new, y, theta));fprintf('Theata: %f\n',theta);% Estimate the price of a 1650 sq-ft, 3 br house% ====================== YOUR CODE HERE ======================% Because of the way meshgrids work in the surf command, we need to % transpose J_vals before calling surf, or else the axes will be flipped%y_vals = y_vals';% Surface plotfigure;if (K==2)  theta0_vals = linspace(-3, 3, 10);  theta1_vals = linspace(-3, 3, 10);% initialize J_vals to a matrix of 0's  y_vals = zeros(length(theta0_vals), length(theta1_vals));% Fill out J_vals  for i = 1:length(theta0_vals)      for j = 1:length(theta1_vals)	    t = [1 theta0_vals(i)  theta1_vals(j)];    	    y_vals(i,j) = t*theta;      end  end  y_vals=y_vals';  surf(theta0_vals, theta1_vals, y_vals);  xlabel('\theta_0'); ylabel('\theta_1');  hold on;  scatter3(Z(:,1),Z(:,2),y);elseif(K==1)  a= (-3:+3)';  plot(a,a.*theta(2)+theta(1),'-');  hold on;  plot(Z,y,'rx','MarkerSize',10);endif%plot(Z(1), Z(2),y, 'rx', 'MarkerSize', 10, 'LineWidth', 2);